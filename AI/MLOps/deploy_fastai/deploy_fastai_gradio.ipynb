{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bdM22ZRJPxf"
      },
      "source": [
        "# Deploy the Fastai Classifier using Gradio and Hugging Face Spaces"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jC_GSmA68_35"
      },
      "source": [
        "# Preparations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VGa9Mfdz2A84",
        "outputId": "3181d897-871a-4483-e20a-bff69de62007"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# connect colab to google drive by mounting\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YMAkq_eq6h_7",
        "outputId": "c3502ea2-9774-46bf-f721-35ddea70c5e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Errno 2] No such file or directory: 'drive/MyDrive/wt23-wastewise/'\n",
            "/content/drive/MyDrive/wt23-wastewise\n"
          ]
        }
      ],
      "source": [
        "# navigate to WasteWise directory\n",
        "%cd drive/MyDrive/wt23-wastewise/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "znQpIppX84-s",
        "outputId": "d9085030-2d37-412b-e7f9-3b6582a5e8a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "remote: Enumerating objects: 20, done.\u001b[K\n",
            "remote: Counting objects:   5% (1/20)\u001b[K\rremote: Counting objects:  10% (2/20)\u001b[K\rremote: Counting objects:  15% (3/20)\u001b[K\rremote: Counting objects:  20% (4/20)\u001b[K\rremote: Counting objects:  25% (5/20)\u001b[K\rremote: Counting objects:  30% (6/20)\u001b[K\rremote: Counting objects:  35% (7/20)\u001b[K\rremote: Counting objects:  40% (8/20)\u001b[K\rremote: Counting objects:  45% (9/20)\u001b[K\rremote: Counting objects:  50% (10/20)\u001b[K\rremote: Counting objects:  55% (11/20)\u001b[K\rremote: Counting objects:  60% (12/20)\u001b[K\rremote: Counting objects:  65% (13/20)\u001b[K\rremote: Counting objects:  70% (14/20)\u001b[K\rremote: Counting objects:  75% (15/20)\u001b[K\rremote: Counting objects:  80% (16/20)\u001b[K\rremote: Counting objects:  85% (17/20)\u001b[K\rremote: Counting objects:  90% (18/20)\u001b[K\rremote: Counting objects:  95% (19/20)\u001b[K\rremote: Counting objects: 100% (20/20)\u001b[K\rremote: Counting objects: 100% (20/20), done.\u001b[K\n",
            "remote: Compressing objects: 100% (12/12), done.\u001b[K\n",
            "remote: Total 20 (delta 9), reused 15 (delta 8), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (20/20), 6.57 MiB | 1.50 MiB/s, done.\n",
            "From https://github.com/TechLabs-Berlin/wt23-wastewise\n",
            "   ce6fda1..4d13a02  MLOps                   -> origin/MLOps\n",
            "   d16396d..0e3e4e2  classifier_fastai       -> origin/classifier_fastai\n",
            "   c52b9b6..0579f18  machine-learning-Marina -> origin/machine-learning-Marina\n",
            "Updating ce6fda1..4d13a02\n",
            "Fast-forward\n",
            " AI/MLOps/deploy_fastai/deploy_fastai_gradio.ipynb | 482 \u001b[32m++++++++++++++\u001b[m\u001b[31m--------\u001b[m\n",
            " 1 file changed, 305 insertions(+), 177 deletions(-)\n"
          ]
        }
      ],
      "source": [
        "# get latest version of branch\n",
        "!git pull"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "al7SccCO4g1O",
        "outputId": "73452e77-058b-4101-f9b6-72a6c951e9f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m719.8/719.8 KB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.7/468.7 KB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m43.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m66.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.1/200.1 KB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m58.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.2/212.2 KB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.9/132.9 KB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 KB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m89.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.6/264.6 KB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.8/158.8 KB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.2/114.2 KB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m55.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# install the fastbook library\n",
        "!pip install -Uqq fastbook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sn-GZDYfJPBO",
        "outputId": "12753656-7233-4f2d-cb83-59e99acbd96e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.7/15.7 MB\u001b[0m \u001b[31m43.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 KB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.8/57.8 KB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.1/57.1 KB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 KB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.1/144.1 KB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.5/50.5 KB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.5/129.5 KB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.9/66.9 KB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.6/69.6 KB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 KB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "# install gradio\n",
        "!pip install -Uqq gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Cv-hq5EB4iMx"
      },
      "outputs": [],
      "source": [
        "# import libraries\n",
        "# fastbook/fastai related libraries: used for training the classifier\n",
        "from fastbook import *\n",
        "\n",
        "# import gradio for deployment \n",
        "import gradio as gr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oRPQYT7wemhW"
      },
      "outputs": [],
      "source": [
        "# disable warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cyTnZ8ButXzl"
      },
      "source": [
        "# Load the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "af0hV7CCKk0o"
      },
      "outputs": [],
      "source": [
        "# load the model\n",
        "learn = load_learner('/content/drive/MyDrive/wastewise_models/resnet101_waste_recogniser_fastai_v2.pkl', cpu=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        },
        "id": "mfrjSyQ1OZki",
        "outputId": "be8a7607-2fda-45f7-fa04-62e5c4b98863"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 342 ms, sys: 7.42 ms, total: 349 ms\n",
            "Wall time: 358 ms\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "('banana_peels',\n",
              " tensor(2),\n",
              " tensor([3.1576e-04, 5.4867e-02, 8.5881e-01, 1.5164e-02, 7.3338e-03, 3.3237e-03, 2.3346e-04, 3.6541e-03, 2.9444e-04, 7.1263e-03, 1.9181e-03, 4.0224e-04, 2.3900e-04, 1.1081e-04, 2.4067e-03, 5.7777e-04,\n",
              "         2.9609e-02, 6.4534e-04, 2.8585e-03, 1.0105e-02]))"
            ]
          },
          "execution_count": 155,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# test model inference on banana peel and see how long it takes\n",
        "%time learn.predict(\"/content/drive/MyDrive/wt23-wastewise/AI/data_20_classes/banana_peels/10_175867801.jpg\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-d1PVG1OsbU"
      },
      "source": [
        "The fine tuned network has 101 layers and I was concerned it would take too long for inference on CPU, but this is definitely reasonable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "M07zLAeEdJtI"
      },
      "outputs": [],
      "source": [
        "# make a \"list\" containing the classes the classifier can distinguish\n",
        "# note that actual data type is not \"list\", but \"fastai.data.transforms.CategoryMap\"\n",
        "# it still works in a comparable way\n",
        "labels = learn.dls.vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pZysg9nKdUwG",
        "outputId": "6510e398-8ebe-4660-9655-582b18b39f99"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'fastai.data.transforms.CategoryMap'>\n",
            "['aluminum_foil', 'apples', 'banana_peels', 'cardboard', 'condoms', 'diapers', 'food_waste', 'glass_bottle', 'old_books', 'oranges', 'pans', 'pizza_box', 'plastic_bags', 'plastic_packaging', 'plastic_toys', 'smartphone', 'tampons', 'tea_bags', 'tetrapack', 'toothbrush']\n",
            "Label at index 2: banana_peels\n"
          ]
        }
      ],
      "source": [
        "# see labels\n",
        "print(type(labels))\n",
        "print(labels)\n",
        "print(\"Label at index 2: \" + labels[2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65WcXx-rLNew"
      },
      "source": [
        "# Make a gradio app that can be used in an API for the real app.\n",
        "\n",
        "In this case, I keep the outputs minimal, because the rest will be solved in JavaScript and HTML together with our WebDev.\n",
        "\n",
        "Just make it classify the image, nothing more.\n",
        "\n",
        "If I include more, it will be difficult for our web dev to integrate it with his proper app."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "F1IE-C-5k0nU"
      },
      "outputs": [],
      "source": [
        "# define a function for the learner\n",
        "def predict(img):\n",
        "    img = PILImage.create(img)\n",
        "    pred,pred_idx,probs = learn.predict(img)\n",
        "\n",
        "    return {labels[i]: float(probs[i]) for i in range(len(labels))}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 621
        },
        "id": "AvGD7VakirDA",
        "outputId": "b6186fde-919a-4c21-b5da-b7d890f719c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Note: opening Chrome Inspector may crash demo inside Colab notebooks.\n",
            "\n",
            "To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "data": {
            "application/javascript": "(async (port, path, width, height, cache, element) => {\n                        if (!google.colab.kernel.accessAllowed && !cache) {\n                            return;\n                        }\n                        element.appendChild(document.createTextNode(''));\n                        const url = await google.colab.kernel.proxyPort(port, {cache});\n\n                        const external_link = document.createElement('div');\n                        external_link.innerHTML = `\n                            <div style=\"font-family: monospace; margin-bottom: 0.5rem\">\n                                Running on <a href=${new URL(path, url).toString()} target=\"_blank\">\n                                    https://localhost:${port}${path}\n                                </a>\n                            </div>\n                        `;\n                        element.appendChild(external_link);\n\n                        const iframe = document.createElement('iframe');\n                        iframe.src = new URL(path, url).toString();\n                        iframe.height = height;\n                        iframe.allow = \"autoplay; camera; microphone; clipboard-read; clipboard-write;\"\n                        iframe.width = width;\n                        iframe.style.border = 0;\n                        element.appendChild(iframe);\n                    })(7860, \"/\", \"100%\", 500, false, window.element)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# make a gradio interface\n",
        "gr.Interface(fn=predict, inputs=gr.Image(shape=(512, 512)), outputs=gr.Label(num_top_classes=3)).launch(share=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54CqcuaD0JW4"
      },
      "source": [
        "# Make a proper gradio website that can be shown off as a standalone\n",
        "\n",
        "The previous example was very basic, but there are so many possibilities in Gradio! Now I will do a little more frontend myself and assemble an actual usable waste recognizer website with gradio."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NHvHFI3f3_fn",
        "outputId": "cb901814-662e-42fc-c182-32827d18d96a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.9/dist-packages (0.19.3)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from scikit-image) (1.4.1)\n",
            "Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.9/dist-packages (from scikit-image) (2.25.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from scikit-image) (23.0)\n",
            "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,!=8.3.0,>=6.1.0 in /usr/local/lib/python3.9/dist-packages (from scikit-image) (8.4.0)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.9/dist-packages (from scikit-image) (3.0)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.9/dist-packages (from scikit-image) (1.10.1)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.9/dist-packages (from scikit-image) (1.22.4)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.9/dist-packages (from scikit-image) (2023.3.21)\n"
          ]
        }
      ],
      "source": [
        "!pip install scikit-image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yTzfcURf4eDW"
      },
      "outputs": [],
      "source": [
        "import skimage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hJVnxjr30jCY"
      },
      "outputs": [],
      "source": [
        "# make dictionaries translating the class to something else for the output\n",
        "\n",
        "# adapt class spelling for output\n",
        "spelling_dict = {\n",
        "    \"aluminum_foil\": \"aluminum foil\",\n",
        "    \"apples\": \"apple\",\n",
        "    \"banana_peels\": \"banana peel\",\n",
        "    \"cardboard\": \"cardboard\",\n",
        "    \"condoms\": \"condom\",\n",
        "    \"diapers\": \"diaper\",\n",
        "    \"food_waste\": \"food waste\",\n",
        "    \"glass_bottle\": \"glass bottle\",\n",
        "    \"old_books\": \"book\",\n",
        "    \"oranges\": \"orange\",\n",
        "    \"pans\": \"pan\",\n",
        "    \"pizza_box\": \"pizza box\",\n",
        "    \"plastic_bags\": \"plastic bag\",\n",
        "    \"plastic_packaging\": \"plastic packaging\",\n",
        "    \"plastic_toys\": \"plastic toy\",\n",
        "    \"smartphone\": \"smartphone\",\n",
        "    \"tampons\": \"tampon\",\n",
        "    \"tea_bags\": \"tea bag\",\n",
        "    \"tetrapack\": \"tetra pak\",\n",
        "    \"toothbrush\": \"toothbrush\"\n",
        "    }\n",
        "\n",
        "\n",
        "    # recommend waste bin for each class\n",
        "bin_dict = {\n",
        "    \"aluminum_foil\": \"gelbe sack\",\n",
        "    \"apples\": \"bio waste\",\n",
        "    \"banana_peels\": \"bio waste\",\n",
        "    \"cardboard\": \"paper waste\",\n",
        "    \"condoms\": \"residual waste\",\n",
        "    \"diapers\": \"residual waste\",\n",
        "    \"food_waste\": \"residual waste\",\n",
        "    \"glass_bottle\": \"glass waste\",\n",
        "    \"old_books\": \"paper waste\",\n",
        "    \"oranges\": \"bio waste\",\n",
        "    \"pans\": \"residual waste\",\n",
        "    \"pizza_box\": \"residual waste\",\n",
        "    \"plastic_bags\": \"plastic waste\",\n",
        "    \"plastic_packaging\": \"plastic waste\",\n",
        "    \"plastic_toys\": \"residual waste\",\n",
        "    \"smartphone\": \"wertstoffsammlung\",\n",
        "    \"tampons\": \"residual waste\",\n",
        "    \"tea_bags\": \"bio waste\",\n",
        "    \"tetrapack\": \"plastic waste\",\n",
        "    \"toothbrush\": \"residual waste\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FWp_mun_9_sK"
      },
      "outputs": [],
      "source": [
        "labels = list(spelling_dict.values())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JBS4L5aA3HNG"
      },
      "outputs": [],
      "source": [
        "# describe the interface\n",
        "title = \"WasteWise\"\n",
        "description = \"Hey! What do you want to throw away? \\n Just take a picture by clicking the camera icon, then hit 'submit'. \\n You will see the result and confidence to the right hand side as well as a recommendation below. \\n To label the image as misclassified, just hit 'Flag'. \\n To see what parts of the image are responsible for the output, hit 'Interpret' and wait for a few. \\n  The parts of the image that contributed to increase the likelihood of the outputted class are marked red. \\n The parts that decrease the class confidence are highlighted blue. \\n The intensity of color corresponds to the importance of that part of the input. \\n At the bottom you can load some example images.\"\n",
        "examples = ['/content/drive/MyDrive/wt23-wastewise/AI/data_20_classes/banana_peels/10_175867801.jpg',\n",
        "            \"/content/drive/MyDrive/wt23-wastewise/AI/data_20_classes/pizza_box/31_can-you-recycle-pizza-boxes-and-know-the-rule-L-R6CB5u.jpeg\",\n",
        "            \"/content/drive/MyDrive/wt23-wastewise/AI/data_20_classes/tea_bags/36_b3dcc3f8f7fe76facafeb7ef2cbeeb5f--the-tea-the-works.jpg\",\n",
        "            \"/content/drive/MyDrive/wt23-wastewise/AI/data_20_classes/plastic_bags/21_recycle-plastic-bags.jpg\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bbLKQVbw0ON1"
      },
      "outputs": [],
      "source": [
        "# define a function for the learner\n",
        "def predict_full(img):\n",
        "    img = PILImage.create(img)\n",
        "    pred,pred_idx,probs = learn.predict(img)\n",
        "\n",
        "    return {labels[i]: float(probs[i]) for i in range(len(labels))}, f'It belongs into the {bin_dict[pred]}.'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 751
        },
        "id": "du6jWgHM1yWT",
        "outputId": "20eb1350-31ec-4e81-ef57-0e6cfc8f9256"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/gradio/outputs.py:197: UserWarning: Usage of gradio.outputs is deprecated, and will not be supported in the future, please import your components from gradio.components\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.9/dist-packages/gradio/deprecation.py:40: UserWarning: The 'type' parameter has been deprecated. Use the Number component instead.\n",
            "  warnings.warn(value)\n",
            "/usr/local/lib/python3.9/dist-packages/gradio/deprecation.py:40: UserWarning: `enable_queue` is deprecated in `Interface()`, please use it within `launch()` instead.\n",
            "  warnings.warn(value)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Note: opening Chrome Inspector may crash demo inside Colab notebooks.\n",
            "\n",
            "To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "data": {
            "application/javascript": "(async (port, path, width, height, cache, element) => {\n                        if (!google.colab.kernel.accessAllowed && !cache) {\n                            return;\n                        }\n                        element.appendChild(document.createTextNode(''));\n                        const url = await google.colab.kernel.proxyPort(port, {cache});\n\n                        const external_link = document.createElement('div');\n                        external_link.innerHTML = `\n                            <div style=\"font-family: monospace; margin-bottom: 0.5rem\">\n                                Running on <a href=${new URL(path, url).toString()} target=\"_blank\">\n                                    https://localhost:${port}${path}\n                                </a>\n                            </div>\n                        `;\n                        element.appendChild(external_link);\n\n                        const iframe = document.createElement('iframe');\n                        iframe.src = new URL(path, url).toString();\n                        iframe.height = height;\n                        iframe.allow = \"autoplay; camera; microphone; clipboard-read; clipboard-write;\"\n                        iframe.width = width;\n                        iframe.style.border = 0;\n                        element.appendChild(iframe);\n                    })(7861, \"/\", \"100%\", 500, false, window.element)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# make a gradio interface\n",
        "gr.Interface(fn=predict_full, inputs = gr.Webcam(shape=(512, 512)), title=title, description=description, examples=examples, interpretation = \"default\", enable_queue = True, outputs=[gr.outputs.Label(num_top_classes=3), \"text\"]).launch(share=False)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "cYk0pFaSSo2S"
      },
      "source": [
        "### A quick word about the interpretation\n",
        "I added a simple CAM interpretation class activation mapping (CAM) to the standalone app. Once the image is classified, the user can can click \"Interpret\". The image is then dissected into squares. The resolution here is rather bad, it's not individual pixels, but square groups of them, but it's still a cool feature that's interactive and available for the user. The squares are colored according to their importance for the classification. Please note that there are other options for this available. For example, a shapley-based interpreter is supported as well and can be used by setting `interpretation = \"shap` as well as adding `shap` to the requirements (or rather installing shap, when you run it locally). This allows for a more detailed interpretation, but comes with __much__ longer run rimes and I figured that user would not like to wait this long."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fpKvBzGr_Y5Q"
      },
      "source": [
        "# Create new Hugginface Space\n",
        "\n",
        "To make the app available online \"permanently\"\n",
        "\n",
        "## app.py\n",
        "\n",
        "This file needs to be saved to \"app.py\".\n",
        "\n",
        "It consists of importing the dependencies, loading the classifier, initializing the predict function and setting up the interface.\n",
        "\n",
        "### For the only-API app\n",
        "```python\n",
        "import gradio as gr\n",
        "from fastai.vision.all import *\n",
        "import skimage\n",
        "\n",
        "# load image classifier\n",
        "learn = load_learner('resnet101_waste_recogniser.pkl')\n",
        "\n",
        "# get classes of waste\n",
        "labels = learn.dls.vocab\n",
        "\n",
        "# define a function for the learner\n",
        "# define a function for the learner\n",
        "def predict(img):\n",
        "    img = PILImage.create(img)\n",
        "    pred,pred_idx,probs = learn.predict(img)\n",
        "\n",
        "    return {labels[i]: float(probs[i]) for i in range(len(labels))}\n",
        "\n",
        "# make a gradio interface\n",
        "gr.Interface(fn=predict, inputs=gr.Image(shape=(512, 512)), outputs=gr.Label(num_top_classes=3)).launch(share=False)\n",
        "```\n",
        "\n",
        "### For the standalone app\n",
        "```python\n",
        "import gradio as gr\n",
        "from fastai.vision.all import *\n",
        "import skimage\n",
        "\n",
        "# load image classifier\n",
        "learn = load_learner('resnet101_waste_recogniser.pkl')\n",
        "\n",
        "# get classes of waste\n",
        "labels = learn.dls.vocab\n",
        "\n",
        "# make dictionaries translating the class to something else for the output\n",
        "# adapt class spelling for output\n",
        "spelling_dict = {\n",
        "    \"aluminum_foil\": \"aluminum foil\",\n",
        "    \"apples\": \"apple\",\n",
        "    \"banana_peels\": \"banana peel\",\n",
        "    \"cardboard\": \"cardboard\",\n",
        "    \"condoms\": \"condom\",\n",
        "    \"diapers\": \"diaper\",\n",
        "    \"food_waste\": \"food waste\",\n",
        "    \"glass_bottle\": \"glass bottle\",\n",
        "    \"old_books\": \"book\",\n",
        "    \"oranges\": \"orange\",\n",
        "    \"pans\": \"pan\",\n",
        "    \"pizza_box\": \"pizza box\",\n",
        "    \"plastic_bags\": \"plastic bag\",\n",
        "    \"plastic_packaging\": \"plastic packaging\",\n",
        "    \"plastic_toys\": \"plastic toy\",\n",
        "    \"smartphone\": \"smartphone\",\n",
        "    \"tampons\": \"tampon\",\n",
        "    \"tea_bags\": \"tea bag\",\n",
        "    \"tetrapack\": \"tetra pak\",\n",
        "    \"toothbrush\": \"toothbrush\"\n",
        "    }\n",
        "# recommend waste bin for each class\n",
        "bin_dict = {\n",
        "    \"aluminum_foil\": \"gelbe sack\",\n",
        "    \"apples\": \"bio waste\",\n",
        "    \"banana_peels\": \"bio waste\",\n",
        "    \"cardboard\": \"paper waste\",\n",
        "    \"condoms\": \"residual waste\",\n",
        "    \"diapers\": \"residual waste\",\n",
        "    \"food_waste\": \"residual waste\",\n",
        "    \"glass_bottle\": \"glass waste\",\n",
        "    \"old_books\": \"paper waste\",\n",
        "    \"oranges\": \"bio waste\",\n",
        "    \"pans\": \"residual waste\",\n",
        "    \"pizza_box\": \"residual waste\",\n",
        "    \"plastic_bags\": \"plastic waste\",\n",
        "    \"plastic_packaging\": \"plastic waste\",\n",
        "    \"plastic_toys\": \"residual waste\",\n",
        "    \"smartphone\": \"wertstoffsammlung\",\n",
        "    \"tampons\": \"residual waste\",\n",
        "    \"tea_bags\": \"bio waste\",\n",
        "    \"tetrapack\": \"plastic waste\",\n",
        "    \"toothbrush\": \"residual waste\"}\n",
        "\n",
        "\n",
        "# define a function for the learner\n",
        "def predict_full(img):\n",
        "    img = PILImage.create(img)\n",
        "    pred,pred_idx,probs = learn.predict(img)\n",
        "\n",
        "    return {labels[i]: float(probs[i]) for i in range(len(labels))}, f'It belongs into the {bin_dict[pred]}.'\n",
        "\n",
        "\n",
        "# describe the interface\n",
        "title = \"WasteWise\"\n",
        "description = \"Hey! What do you want to throw away? \\n Just take a picture by clicking the camera icon, then hit 'submit'. \\n You will see the result and confidence to the right hand side as well as a recommendation below. \\n To label the image as misclassified, just hit 'Flag'. \\n To see what parts of the image are responsible for the output, hit 'Interpret' and wait for a few seconds. \\n  The parts of the image that contributed to increase the likelihood of the outputted class are marked red. \\n The parts that decrease the class confidence are highlighted blue. \\n The intensity of color corresponds to the importance of that part of the input. \\n At the bottom you can load some example images.\"\n",
        "# examples = [...] # enter examples and path here, save them in GitHub\n",
        "\n",
        "# make a gradio interface\n",
        "gr.Interface(fn=predict_full, inputs = gr.Webcam(shape=(512, 512)), title=title, description=description, interpretation = \"default\", enable_queue = True, outputs=[gr.outputs.Label(num_top_classes=3), \"text\"]).launch()\n",
        "\n",
        "```\n",
        "## requirements.txt\n",
        "\n",
        "This file need to be saved to \"requirements.txt\".\n",
        "\n",
        "It contains the packages the machine needs to install.\n",
        "```\n",
        "fastai\n",
        "scikit-image\n",
        "```\n",
        "\n",
        "## Upload the files\n",
        "\n",
        "1. Create an account at Hugging Face 🤗\n",
        "2. Navigate to \"Spaces\" and click \"Create new Space\"\n",
        "3. Set its name, license etc. and select \"Gradio\".\n",
        "4. Launch\n",
        "5. This will create a remote git repository. Clone it to your local.\n",
        "6. Initialise git large file storage. For this, if on Mac, install Homebrew, navigate to the repo and type \"git install lfs\". It allows you to upload files larger than 100 MB, most importantly your trained model.\n",
        "7. Move all relevant files (app.py, requirements.txt, <your_model>.pkl to that repo.\n",
        "8. Add, commit, push, wait.\n",
        "9. Once building is done, the app can be used.\n",
        "\n",
        "## The deployed apps can be found here\n",
        "\n",
        "#### Standalone version\n",
        "https://huggingface.co/spaces/fabianjkrueger/WasteWise\n",
        "\n",
        "#### API version\n",
        "https://huggingface.co/spaces/fabianjkrueger/WasteWise_API\n",
        "\n",
        "## Usage via API\n",
        "\n",
        "1. Follow one of the links. Both can be used via an API, but the API version is much better suited for integration with our JavaScript app, hence the name.\n",
        "2. Upon opening the link, you can find \"Use via API\" at the very bottom. Open that, then just switch to JavaScript to see the instructions.\n",
        "3. Follow the instructions. Images need to be converted to base 64 strings in order to be passed. Passing the converted string to a variable, then passing the string to the request is strongly recommended.\n",
        "\n",
        "## More cool ideas\n",
        "Another feature that can be activated is to flag the images as misclassified. They then get saved and can be reviewed and reused for training. This is very powerful once the app is properly deployed and used. Currently, however, since the Huggingface Spaces are public, flagged images are visible to everyone in the internet, which comes with some serious data privacy concerns. Your personal waste is not a matter for everyone to see, which is why flagging currently is activated. But as mentioned previously, in case the app ever goes into serious production, we might implement it in combination with data privacy measure.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "6bdM22ZRJPxf"
      ],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
