{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bdM22ZRJPxf"
      },
      "source": [
        "# Deploy the Fastai Classifier using Gradio and Hugging Face Spaces"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jC_GSmA68_35"
      },
      "source": [
        "# Preparations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VGa9Mfdz2A84",
        "outputId": "98116cf4-70f7-4869-9a05-f048866b39f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# connect colab to google drive by mounting\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YMAkq_eq6h_7",
        "outputId": "53e7aba3-8ba3-4ce7-c501-1cf685844059"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/wt23-wastewise\n"
          ]
        }
      ],
      "source": [
        "# navigate to WasteWise directory\n",
        "%cd drive/MyDrive/wt23-wastewise/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "znQpIppX84-s",
        "outputId": "203bce39-3639-410b-92d0-f2570e08111c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "remote: Enumerating objects: 21, done.\u001b[K\n",
            "remote: Counting objects:   4% (1/21)\u001b[K\rremote: Counting objects:   9% (2/21)\u001b[K\rremote: Counting objects:  14% (3/21)\u001b[K\rremote: Counting objects:  19% (4/21)\u001b[K\rremote: Counting objects:  23% (5/21)\u001b[K\rremote: Counting objects:  28% (6/21)\u001b[K\rremote: Counting objects:  33% (7/21)\u001b[K\rremote: Counting objects:  38% (8/21)\u001b[K\rremote: Counting objects:  42% (9/21)\u001b[K\rremote: Counting objects:  47% (10/21)\u001b[K\rremote: Counting objects:  52% (11/21)\u001b[K\rremote: Counting objects:  57% (12/21)\u001b[K\rremote: Counting objects:  61% (13/21)\u001b[K\rremote: Counting objects:  66% (14/21)\u001b[K\rremote: Counting objects:  71% (15/21)\u001b[K\rremote: Counting objects:  76% (16/21)\u001b[K\rremote: Counting objects:  80% (17/21)\u001b[K\rremote: Counting objects:  85% (18/21)\u001b[K\rremote: Counting objects:  90% (19/21)\u001b[K\rremote: Counting objects:  95% (20/21)\u001b[K\rremote: Counting objects: 100% (21/21)\u001b[K\rremote: Counting objects: 100% (21/21), done.\u001b[K\n",
            "remote: Compressing objects:   6% (1/16)\u001b[K\rremote: Compressing objects:  12% (2/16)\u001b[K\rremote: Compressing objects:  18% (3/16)\u001b[K\rremote: Compressing objects:  25% (4/16)\u001b[K\rremote: Compressing objects:  31% (5/16)\u001b[K\rremote: Compressing objects:  37% (6/16)\u001b[K\rremote: Compressing objects:  43% (7/16)\u001b[K\rremote: Compressing objects:  50% (8/16)\u001b[K\rremote: Compressing objects:  56% (9/16)\u001b[K\rremote: Compressing objects:  62% (10/16)\u001b[K\rremote: Compressing objects:  68% (11/16)\u001b[K\rremote: Compressing objects:  75% (12/16)\u001b[K\rremote: Compressing objects:  81% (13/16)\u001b[K\rremote: Compressing objects:  87% (14/16)\u001b[K\rremote: Compressing objects:  93% (15/16)\u001b[K\rremote: Compressing objects: 100% (16/16)\u001b[K\rremote: Compressing objects: 100% (16/16), done.\u001b[K\n",
            "remote: Total 21 (delta 8), reused 12 (delta 5), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (21/21), 9.95 KiB | 2.00 KiB/s, done.\n",
            "From https://github.com/TechLabs-Berlin/wt23-wastewise\n",
            "   d9eccbd..ce6fda1  MLOps      -> origin/MLOps\n",
            "   4643ef9..4e07681  main       -> origin/main\n",
            "Updating d9eccbd..ce6fda1\n",
            "Fast-forward\n",
            " AI/MLOps/deploy_fastai/deploy_fastai_gradio.ipynb  | 635 \u001b[32m+++++++++++++++++++++\u001b[m\n",
            " .../deploy_fastai/deploy_fastai_notebook.ipynb     |  55 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
            " 2 files changed, 669 insertions(+), 21 deletions(-)\n",
            " create mode 100644 AI/MLOps/deploy_fastai/deploy_fastai_gradio.ipynb\n"
          ]
        }
      ],
      "source": [
        "# get latest version of branch\n",
        "!git pull"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "al7SccCO4g1O"
      },
      "outputs": [],
      "source": [
        "# install the fastbook library\n",
        "!pip install -Uqq fastbook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "sn-GZDYfJPBO"
      },
      "outputs": [],
      "source": [
        "# install gradio\n",
        "!pip install -Uqq gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Cv-hq5EB4iMx"
      },
      "outputs": [],
      "source": [
        "# import libraries\n",
        "# fastbook/fastai related libraries: used for training the classifier\n",
        "from fastbook import *\n",
        "\n",
        "# import gradio for deployment \n",
        "import gradio as gr"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# disable warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "oRPQYT7wemhW"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the model"
      ],
      "metadata": {
        "id": "cyTnZ8ButXzl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "af0hV7CCKk0o"
      },
      "outputs": [],
      "source": [
        "# load the model\n",
        "learn = load_learner('/content/drive/MyDrive/wastewise_models/resnet101_waste_recogniser_fastai_v2.pkl', cpu=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test model inference on banana peel and see how long it takes\n",
        "%time learn.predict(\"/content/drive/MyDrive/wt23-wastewise/AI/data_20_classes/banana_peels/10_175867801.jpg\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        },
        "id": "mfrjSyQ1OZki",
        "outputId": "be8a7607-2fda-45f7-fa04-62e5c4b98863"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 342 ms, sys: 7.42 ms, total: 349 ms\n",
            "Wall time: 358 ms\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('banana_peels',\n",
              " tensor(2),\n",
              " tensor([3.1576e-04, 5.4867e-02, 8.5881e-01, 1.5164e-02, 7.3338e-03, 3.3237e-03, 2.3346e-04, 3.6541e-03, 2.9444e-04, 7.1263e-03, 1.9181e-03, 4.0224e-04, 2.3900e-04, 1.1081e-04, 2.4067e-03, 5.7777e-04,\n",
              "         2.9609e-02, 6.4534e-04, 2.8585e-03, 1.0105e-02]))"
            ]
          },
          "metadata": {},
          "execution_count": 155
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The fine tuned network has 101 layers and I was concerned it would take too long for inference on CPU, but this is definitely reasonable."
      ],
      "metadata": {
        "id": "z-d1PVG1OsbU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# make a \"list\" containing the classes the classifier can distinguish\n",
        "# note that actual data type is not \"list\", but \"fastai.data.transforms.CategoryMap\"\n",
        "# it still works in a comparable way\n",
        "labels = learn.dls.vocab"
      ],
      "metadata": {
        "id": "M07zLAeEdJtI"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# see labels\n",
        "print(type(labels))\n",
        "print(labels)\n",
        "print(\"Label at index 2: \" + labels[2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pZysg9nKdUwG",
        "outputId": "6510e398-8ebe-4660-9655-582b18b39f99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'fastai.data.transforms.CategoryMap'>\n",
            "['aluminum_foil', 'apples', 'banana_peels', 'cardboard', 'condoms', 'diapers', 'food_waste', 'glass_bottle', 'old_books', 'oranges', 'pans', 'pizza_box', 'plastic_bags', 'plastic_packaging', 'plastic_toys', 'smartphone', 'tampons', 'tea_bags', 'tetrapack', 'toothbrush']\n",
            "Label at index 2: banana_peels\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65WcXx-rLNew"
      },
      "source": [
        "# Make a gradio app that can be used in an API for the real app.\n",
        "\n",
        "In this case, I keep the outputs minimal, because the rest will be solved in JavaScript and HTML together with our WebDev.\n",
        "\n",
        "Just make it classify the image, nothing more.\n",
        "\n",
        "If I include more, it will be difficult for our web dev to integrate it with his proper app."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# define a function for the learner\n",
        "def predict(img):\n",
        "    img = PILImage.create(img)\n",
        "    pred,pred_idx,probs = learn.predict(img)\n",
        "\n",
        "    return {labels[i]: float(probs[i]) for i in range(len(labels))}"
      ],
      "metadata": {
        "id": "F1IE-C-5k0nU"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# make a gradio interface\n",
        "gr.Interface(fn=predict, inputs=gr.Image(shape=(512, 512)), outputs=gr.Label(num_top_classes=3)).launch(share=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 621
        },
        "id": "AvGD7VakirDA",
        "outputId": "12adbccd-3fea-4ef3-e6cf-eba005d54c6e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Note: opening Chrome Inspector may crash demo inside Colab notebooks.\n",
            "\n",
            "To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "(async (port, path, width, height, cache, element) => {\n",
              "                        if (!google.colab.kernel.accessAllowed && !cache) {\n",
              "                            return;\n",
              "                        }\n",
              "                        element.appendChild(document.createTextNode(''));\n",
              "                        const url = await google.colab.kernel.proxyPort(port, {cache});\n",
              "\n",
              "                        const external_link = document.createElement('div');\n",
              "                        external_link.innerHTML = `\n",
              "                            <div style=\"font-family: monospace; margin-bottom: 0.5rem\">\n",
              "                                Running on <a href=${new URL(path, url).toString()} target=\"_blank\">\n",
              "                                    https://localhost:${port}${path}\n",
              "                                </a>\n",
              "                            </div>\n",
              "                        `;\n",
              "                        element.appendChild(external_link);\n",
              "\n",
              "                        const iframe = document.createElement('iframe');\n",
              "                        iframe.src = new URL(path, url).toString();\n",
              "                        iframe.height = height;\n",
              "                        iframe.allow = \"autoplay; camera; microphone; clipboard-read; clipboard-write;\"\n",
              "                        iframe.width = width;\n",
              "                        iframe.style.border = 0;\n",
              "                        element.appendChild(iframe);\n",
              "                    })(7861, \"/\", \"100%\", 500, false, window.element)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Make a proper gradio website that can be shown off as a standalone\n",
        "\n",
        "The previous example was very basic, but there are so many possibilities in Gradio! Now I will do a little more frontend myself and assemble an actual usable waste recognizer website with gradio."
      ],
      "metadata": {
        "id": "54CqcuaD0JW4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-image"
      ],
      "metadata": {
        "id": "NHvHFI3f3_fn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import skimage"
      ],
      "metadata": {
        "id": "yTzfcURf4eDW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# make dictionaries translating the class to something else for the output\n",
        "\n",
        "# adapt class spelling for output\n",
        "spelling_dict = {\n",
        "    \"aluminum_foil\": \"aluminum foil\",\n",
        "    \"apples\": \"apple\",\n",
        "    \"banana_peels\": \"banana peel\",\n",
        "    \"cardboard\": \"cardboard\",\n",
        "    \"condoms\": \"condom\",\n",
        "    \"diapers\": \"diaper\",\n",
        "    \"food_waste\": \"food waste\",\n",
        "    \"glass_bottle\": \"glass bottle\",\n",
        "    \"old_books\": \"book\",\n",
        "    \"oranges\": \"orange\",\n",
        "    \"pans\": \"pan\",\n",
        "    \"pizza_box\": \"pizza box\",\n",
        "    \"plastic_bags\": \"plastic bag\",\n",
        "    \"plastic_packaging\": \"plastic packaging\",\n",
        "    \"plastic_toys\": \"plastic toy\",\n",
        "    \"smartphone\": \"smartphone\",\n",
        "    \"tampons\": \"tampon\",\n",
        "    \"tea_bags\": \"tea bag\",\n",
        "    \"tetrapack\": \"tetra pak\",\n",
        "    \"toothbrush\": \"toothbrush\"\n",
        "    }\n",
        "\n",
        "\n",
        "    # recommend waste bin for each class\n",
        "bin_dict = {\n",
        "    \"aluminum_foil\": \"gelbe sack\",\n",
        "    \"apples\": \"bio waste\",\n",
        "    \"banana_peels\": \"bio waste\",\n",
        "    \"cardboard\": \"paper waste\",\n",
        "    \"condoms\": \"residual waste\",\n",
        "    \"diapers\": \"residual waste\",\n",
        "    \"food_waste\": \"residual waste\",\n",
        "    \"glass_bottle\": \"glass waste\",\n",
        "    \"old_books\": \"paper waste\",\n",
        "    \"oranges\": \"bio waste\",\n",
        "    \"pans\": \"residual waste\",\n",
        "    \"pizza_box\": \"residual waste\",\n",
        "    \"plastic_bags\": \"plastic waste\",\n",
        "    \"plastic_packaging\": \"plastic waste\",\n",
        "    \"plastic_toys\": \"residual waste\",\n",
        "    \"smartphone\": \"wertstoffsammlung\",\n",
        "    \"tampons\": \"residual waste\",\n",
        "    \"tea_bags\": \"bio waste\",\n",
        "    \"tetrapack\": \"plastic waste\",\n",
        "    \"toothbrush\": \"residual waste\"}"
      ],
      "metadata": {
        "id": "hJVnxjr30jCY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = list(spelling_dict.values())"
      ],
      "metadata": {
        "id": "FWp_mun_9_sK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# describe the interface\n",
        "title = \"WasteWise\"\n",
        "description = \"Hey! What do you want to throw away? \\n Just take a picture by clicking the camera icon, then hit 'submit'. \\n You will see the result and confidence to the right hand side as well as a recommendation below. \\n To label the image as misclassified, just hit 'Flag'. \\n To see what parts of the image are responsible for the output, hit 'Interpret' and wait for a few. \\n  The parts of the image that contributed to increase the likelihood of the outputted class are marked red. \\n The parts that decrease the class confidence are highlighted blue. \\n The intensity of color corresponds to the importance of that part of the input. \\n At the bottom you can load some example images.\"\n",
        "examples = ['/content/drive/MyDrive/wt23-wastewise/AI/data_20_classes/banana_peels/10_175867801.jpg',\n",
        "            \"/content/drive/MyDrive/wt23-wastewise/AI/data_20_classes/pizza_box/31_can-you-recycle-pizza-boxes-and-know-the-rule-L-R6CB5u.jpeg\",\n",
        "            \"/content/drive/MyDrive/wt23-wastewise/AI/data_20_classes/tea_bags/36_b3dcc3f8f7fe76facafeb7ef2cbeeb5f--the-tea-the-works.jpg\",\n",
        "            \"/content/drive/MyDrive/wt23-wastewise/AI/data_20_classes/plastic_bags/21_recycle-plastic-bags.jpg\"]"
      ],
      "metadata": {
        "id": "JBS4L5aA3HNG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define a function for the learner\n",
        "def predict_full(img):\n",
        "    img = PILImage.create(img)\n",
        "    pred,pred_idx,probs = learn.predict(img)\n",
        "\n",
        "    return {labels[i]: float(probs[i]) for i in range(len(labels))}, f'It belongs into the {bin_dict[pred]}.'"
      ],
      "metadata": {
        "id": "bbLKQVbw0ON1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# make a gradio interface\n",
        "gr.Interface(fn=predict_full, inputs = gr.Webcam(shape=(512, 512)), title=title, description=description, examples=examples, interpretation = \"default\", enable_queue = True, outputs=[gr.outputs.Label(num_top_classes=3), \"text\"]).launch(share=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 621
        },
        "id": "du6jWgHM1yWT",
        "outputId": "2afeb1cd-42a5-497d-b2c0-bd502155ffd8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Note: opening Chrome Inspector may crash demo inside Colab notebooks.\n",
            "\n",
            "To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "(async (port, path, width, height, cache, element) => {\n",
              "                        if (!google.colab.kernel.accessAllowed && !cache) {\n",
              "                            return;\n",
              "                        }\n",
              "                        element.appendChild(document.createTextNode(''));\n",
              "                        const url = await google.colab.kernel.proxyPort(port, {cache});\n",
              "\n",
              "                        const external_link = document.createElement('div');\n",
              "                        external_link.innerHTML = `\n",
              "                            <div style=\"font-family: monospace; margin-bottom: 0.5rem\">\n",
              "                                Running on <a href=${new URL(path, url).toString()} target=\"_blank\">\n",
              "                                    https://localhost:${port}${path}\n",
              "                                </a>\n",
              "                            </div>\n",
              "                        `;\n",
              "                        element.appendChild(external_link);\n",
              "\n",
              "                        const iframe = document.createElement('iframe');\n",
              "                        iframe.src = new URL(path, url).toString();\n",
              "                        iframe.height = height;\n",
              "                        iframe.allow = \"autoplay; camera; microphone; clipboard-read; clipboard-write;\"\n",
              "                        iframe.width = width;\n",
              "                        iframe.style.border = 0;\n",
              "                        element.appendChild(iframe);\n",
              "                    })(7918, \"/\", \"100%\", 500, false, window.element)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 164
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### A quick word about the interpretation: \n",
        "To see some explanation about, see the description in the gradio interface. Please note that there are other options for this available. For example, a shapley-based interpreter is supported as well and can be used by setting `interpretation = \"shap` as well as adding `shap` to the requirements (or rather installing shap, when you run it locally). This allows for a more detailed interpretation, but comes with __much__ longer run rimes and I figured that user would not like to wait this long."
      ],
      "metadata": {
        "id": "cYk0pFaSSo2S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create new Hugginface Space\n",
        "\n",
        "To make the app available online \"permanently\"\n",
        "\n",
        "## app.py\n",
        "\n",
        "\n",
        "This file needs to be saved to \"app.py\"\n",
        "\n",
        "It consists of importing the dependencies, loading the classifier, initializing the predict function and setting up the interface.\n",
        "\n",
        "```python\n",
        "import gradio as gr\n",
        "from fastai.vision.all import *\n",
        "import skimage\n",
        "\n",
        "# load image classifier\n",
        "learn = load_learner('waste_recogniser.pkl')\n",
        "\n",
        "# get classes of waste\n",
        "labels = learn.dls.vocab\n",
        "\n",
        "# make dictionaries translating the class to something else for the output\n",
        "# adapt class spelling for output\n",
        "spelling_dict = {\n",
        "    \"aluminum_foil\": \"aluminum foil\",\n",
        "    \"apples\": \"apple\",\n",
        "    \"banana_peels\": \"banana peel\",\n",
        "    \"cardboard\": \"cardboard\",\n",
        "    \"condoms\": \"condom\",\n",
        "    \"diapers\": \"diaper\",\n",
        "    \"food_waste\": \"food waste\",\n",
        "    \"glass_bottle\": \"glass bottle\",\n",
        "    \"old_books\": \"book\",\n",
        "    \"oranges\": \"orange\",\n",
        "    \"pans\": \"pan\",\n",
        "    \"pizza_box\": \"pizza box\",\n",
        "    \"plastic_bags\": \"plastic bag\",\n",
        "    \"plastic_packaging\": \"plastic packaging\",\n",
        "    \"plastic_toys\": \"plastic toy\",\n",
        "    \"smartphone\": \"smartphone\",\n",
        "    \"tampons\": \"tampon\",\n",
        "    \"tea_bags\": \"tea bag\",\n",
        "    \"tetrapack\": \"tetra pak\",\n",
        "    \"toothbrush\": \"toothbrush\"\n",
        "    }\n",
        "# recommend waste bin for each class\n",
        "bin_dict = {\n",
        "    \"aluminum_foil\": \"gelbe sack\",\n",
        "    \"apples\": \"bio waste\",\n",
        "    \"banana_peels\": \"bio waste\",\n",
        "    \"cardboard\": \"paper waste\",\n",
        "    \"condoms\": \"residual waste\",\n",
        "    \"diapers\": \"residual waste\",\n",
        "    \"food_waste\": \"residual waste\",\n",
        "    \"glass_bottle\": \"glass waste\",\n",
        "    \"old_books\": \"paper waste\",\n",
        "    \"oranges\": \"bio waste\",\n",
        "    \"pans\": \"residual waste\",\n",
        "    \"pizza_box\": \"residual waste\",\n",
        "    \"plastic_bags\": \"plastic waste\",\n",
        "    \"plastic_packaging\": \"plastic waste\",\n",
        "    \"plastic_toys\": \"residual waste\",\n",
        "    \"smartphone\": \"wertstoffsammlung\",\n",
        "    \"tampons\": \"residual waste\",\n",
        "    \"tea_bags\": \"bio waste\",\n",
        "    \"tetrapack\": \"plastic waste\",\n",
        "    \"toothbrush\": \"residual waste\"}\n",
        "\n",
        "\n",
        "# define a function for the learner\n",
        "def predict_full(img):\n",
        "    img = PILImage.create(img)\n",
        "    pred,pred_idx,probs = learn.predict(img)\n",
        "\n",
        "    return {labels[i]: float(probs[i]) for i in range(len(labels))}, f'It belongs into the {bin_dict[pred]}.'\n",
        "\n",
        "\n",
        "# describe the interface\n",
        "title = \"WasteWise\"\n",
        "description = \"Hey! What do you want to throw away? \\n Just take a picture by clicking the camera icon, then hit 'submit'. \\n You will see the result and confidence to the right hand side as well as a recommendation below. \\n To label the image as misclassified, just hit 'Flag'. \\n To see what parts of the image are responsible for the output, hit 'Interpret' and wait for a few seconds. \\n  The parts of the image that contributed to increase the likelihood of the outputted class are marked red. \\n The parts that decrease the class confidence are highlighted blue. \\n The intensity of color corresponds to the importance of that part of the input. \\n At the bottom you can load some example images.\"\n",
        "examples = [...] # enter examples and path here, save them in GitHub\n",
        "\n",
        "# make a gradio interface\n",
        "gr.Interface(fn=predict_full, inputs = gr.Webcam(shape=(512, 512)), title=title, description=description, examples=examples, interpretation = \"default\", enable_queue = True, outputs=[gr.outputs.Label(num_top_classes=3), \"text\"]).launch()\n",
        "```\n",
        "\n",
        "## requirements.txt\n",
        "\n",
        "This needs to be saved in a file as requirements.txt\n",
        "\n",
        "It contains all the libraries needed to run app.py.\n",
        "\n",
        "```\n",
        "fastai\n",
        "scikit-image\n",
        "```\n",
        "\n",
        "## Upload the files\n",
        "\n",
        "1. Create an account at Hugging Face ðŸ¤—\n",
        "2. Navigate to \"Spaces\" and click \"Create new Space\"\n",
        "3. Set its name, license etc. and select \"Gradio\".\n",
        "4. Launch\n",
        "5. This will create a remote git repository. Clone it to your local.\n",
        "6. Move all relevant files (app.py, requirements.txt, <your_model>.pkl to that repo.\n",
        "7. Add, commit, push, wait.\n"
      ],
      "metadata": {
        "id": "fpKvBzGr_Y5Q"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "6bdM22ZRJPxf"
      ]
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}